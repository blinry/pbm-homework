\documentclass[10pt,DIV10,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{microtype}

\title{PMS -- Exercise Sheet 8}
\date{}

\begin{document}

\maketitle

\section*{Exercise 1}

\subsection*{Initialization}

\(A=\left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right)\)

\(b=\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)\)

\(x_0=\left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\)

\subsection*{First iteration}

\(d_0=b - A * x_0 = \left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)-\left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right)*\left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)=\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)\)

\(r_0=d_0=\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)\)

\(\alpha _0=\frac{r_0{}^T* r_0}{d_0{}^T*A *d_0}=\frac{\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)^T* \left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)}{\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)^T* \left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right)*\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right) }=\frac{1}{5}\)

\(x_1= x_0+\alpha _0d_0= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right) + \frac{1}{5}\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right) = \left(
\begin{array}{c}
 \frac{6}{5} \\
 \frac{6}{5} \\
 \frac{6}{5} \\
\end{array}
\right)\)

\subsection*{Second iteration}

\(r_1=r_0- \alpha _0 A * d_0=\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)-\frac{1}{5} \left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right) *\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right) = \left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right)\)

\(\beta _1= \frac{r_1{}^Tr_1}{r_0{}^Tr_0}=\frac{\left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right)^T* \left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right)}{\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)^T* \left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)}=\frac{2}{25}\)

\(d_1= r_1+\beta _1d_0=  \left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right) + \frac{2}{25}\left(
\begin{array}{c}
 6 \\
 6 \\
 6 \\
\end{array}
\right)=\left(
\begin{array}{c}
 -\frac{18}{25} \\
 \frac{72}{25} \\
 -\frac{18}{25} \\
\end{array}
\right)\)

\(\alpha _1=\frac{r_1{}^T* r_1}{d_1{}^T* A * d_1}=\frac{\left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right)^T* \left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right)}{\left(
\begin{array}{c}
 -\frac{18}{25} \\
 \frac{72}{25} \\
 -\frac{18}{25} \\
\end{array}
\right)^T* \left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right) *\left(
\begin{array}{c}
 -\frac{18}{25} \\
 \frac{72}{25} \\
 -\frac{18}{25} \\
\end{array}
\right)}= \frac{5}{3}\)

\(x_2=x_1+\alpha _1d_1= \left(
\begin{array}{c}
 \frac{6}{5} \\
 \frac{6}{5} \\
 \frac{6}{5} \\
\end{array}
\right) + \frac{5}{3}\left(
\begin{array}{c}
 -\frac{18}{25} \\
 \frac{72}{25} \\
 -\frac{18}{25} \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 6 \\
 0 \\
\end{array}
\right)\text{}\)

\(r_2=r_1-\alpha _1 A * d_1= \left(
\begin{array}{c}
 -\frac{6}{5} \\
 \frac{12}{5} \\
 -\frac{6}{5} \\
\end{array}
\right) - \frac{5}{3}\left(
\begin{array}{ccc}
 1 & 1 & 4 \\
 1 & 1 & 1 \\
 4 & 1 & 1 \\
\end{array}
\right) * \left(
\begin{array}{c}
 -\frac{18}{25} \\
 \frac{72}{25} \\
 -\frac{18}{25} \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\)

Stop, residuum \(r_2=0 \Rightarrow\) the calculated optimum \(x_2=\left(
\begin{array}{c}
 0 \\
 6 \\
 0 \\
\end{array}
\right)\)
will not change anymore.

\section*{Exercise 2}

\begin{itemize}
    \item[a)] As each step of the algorithm finds the local minimum of one dimension, for a 3-dimensional systems it will at most take three iterations.
    \item[b)] In large systems, the conjugate gradient algorithm allows for fast approximation, doing only a few iterations until the solution is “good enough”. Gauss elimination does not allow this.

        Additionally, it is sufficient to know the product $Ax$ (in physical calculations, this equals calculating forces). This is an advantage over other approaches, where $A$ has to be known explicitly, especially when $A$ is very sparse.
    \item[c)] $A$ has to be a symmetric, positive-definite matrix. All symmetric matrices are also square.
    \item[d)] 
    \begin{itemize}
    \item $A$ negative-definite $\Rightarrow$ global maximum can be found with CG. In our opinion, the algorithm will work nevertheless, as $d_i^TAd_i$ will be negative, which leads to ascending steps.
    \item According to the lecture: $A$ singular positive indefinite $\Rightarrow$ No unique minimum. CG calculates just one possible Minimum. (Note: We are not sure whether positive {\em in}definite matrices even exist\dots)
    \item $A$ indefinite $\Rightarrow$ saddlepoint problem. CG won't work (or one step-size should be $\infty$, because one can find a point at which one can walk indefinitely and still get smaller objective function values).
    \item $A$ not square $\Rightarrow$ if $A \in \mathbb{R}^{m\times n}$ with $m > n$ ($A$ has more rows than colums) this is a {\em linear curve fitting problem}. This can be solved by decomposing $A$ in a specific way and solving another system of linear equations (which can again be done using CG).
    \end{itemize}
\end{itemize}

\end{document}
